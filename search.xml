<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>全连接神经网络实现线性回归(Tensorflow)</title>
      <link href="/2020/04/02/nnLinerRegression/"/>
      <url>/2020/04/02/nnLinerRegression/</url>
      
        <content type="html"><![CDATA[<pre><code>import tensorflow as tfimport numpy as npimport matplotlib.pyplot as plt#随机生成一系列数据x_data=np.linspace(-0.5,0.5,100)[:,np.newaxis]y_data=x_data**2+np.random.normal(0, 0.02, x_data.shape)x=tf.placeholder(tf.float32,[None,1])y=tf.placeholder(tf.float32,[None,1])#定义两层神经网络w1=tf.Variable(tf.random.normal([1,10]))b1=tf.Variable(tf.ones([1,10]))wx1=tf.matmul(x,w1)+b1act1=tf.nn.tanh(wx1)w2=tf.Variable(tf.random.normal([10,1]))b2=tf.Variable(tf.ones([1,1]))wx2=tf.matmul(act1,w2)+b2pred=tf.nn.tanh(wx2)#定义损失loss=tf.reduce_mean(tf.square(y-pred))#梯度下降优化optimizer=tf.train.GradientDescentOptimizer(0.1)train=optimizer.minimize(loss)init=tf.global_variables_initializer()with tf.Session() as sess:    #训练1000次    sess.run(init)    for step in range(1000):        sess.run(train,feed_dict={x:x_data,y:y_data})        print(&quot;第%d次：&quot;%step)    y_pred=sess.run(pred,feed_dict={x:x_data})    plt.figure(figsize=(10,6))    plt.scatter(x_data,y_data)    plt.plot(x_data,y_pred,color=&quot;red&quot;,linewidth=2)    plt.show()</code></pre><p><img src="https://s1.ax1x.com/2020/04/02/GtpQRP.png" alt="GtpQRP.png"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>LinerRegression</title>
      <link href="/2020/04/02/LinerRegression/"/>
      <url>/2020/04/02/LinerRegression/</url>
      
        <content type="html"><![CDATA[<pre><code>import tensorflow as tfimport numpy as npimport matplotlib.pyplot as pltx_data=[]y_data=[]#随机生成测试数据for i in range(100):    x=np.random.normal(0.0,0.55)    y=x*0.3+0.2+np.random.normal(0.0,0.03)    x_data.append(x)    y_data.append(y)plt.scatter(x_data,y_data,color=&apos;b&apos;,label=&quot;数据&quot;)plt.xlabel(&quot;time&quot;)plt.ylabel(&quot;score&quot;)plt.show()</code></pre><p><img src="https://s1.ax1x.com/2020/04/02/GJtWuT.png" alt="GJtWuT.png">  </p><pre><code>#初始化权重参数值weights=tf.Variable(tf.random.uniform([1],-1.0,1.0))bais=tf.Variable(tf.zeros([1]))y=weights*x_data+bais#定义损失loss=tf.reduce_mean(tf.square(y-y_data))#梯度下降优化optimizer=tf.train.GradientDescentOptimizer(0.25)train=optimizer.minimize(loss)init=tf.initialize_all_variables()sess=tf.Session()sess.run(init)#训练100次for step in range(100):    sess.run(train)    print(step,sess.run(weights),sess.run(bais))#绘制拟合的直线y_pred=w*x_data+bplt.figure(figsize=(10,6))plt.scatter(x_data,y_data)plt.plot(x_data,y_pred,color=&quot;red&quot;,linewidth=2)plt.show()</code></pre><p><img src="https://s1.ax1x.com/2020/04/02/GJtgg0.png" alt="GJtgg0.png"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RNN循环神经网络</title>
      <link href="/2020/03/08/RNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2020/03/08/RNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<pre><code>#初始化参数import numpy as npx=[0.8,0.1]init_data=[0.3,0.6]W=np.asarray([[0.2,0.4],[0.7,0.3]])U=np.asarray([0.8,0.1])b_h=np.asarray([0.2,0.1])V=np.asarray([0.5,0.5])b_o=0.1#进行两次循环for i in range(len(x)):    before_activate=(np.dot(init_data,W)+x[i]*U)+b_h    state=np.tanh(before_activate)    init_data=state    output=np.dot(state,V)+b_o    print(&quot;t%s时刻状态值:%s,输出值:%s&quot;%(i+1,state,output))</code></pre>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FFA-Net: Feature Fusion Attention Network for Single Image Dehazing</title>
      <link href="/2020/02/20/paper/"/>
      <url>/2020/02/20/paper/</url>
      
        <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>在这篇论文当中，作者提出了一种端到端特征融合注意力网络（FFA-Net），可以直接将受天气影响拍摄的照片和朦胧的照片恢复到原本干净的图像。该结构可以保留浅层信息并将其传递到深层。最重要的是，在将所有特征输入特征融合模块之前，FFA-Net对不同级别的特征赋予不同的权重，该权重是通过自适应学习FA模块获得的。它比直接指定权重的要好得多。<br>FFA网络的输入是一个模糊的图像，它被传递到一个浅层特征提取部分，然后被输入到N个具有多跳连接的群结构中，然后通过我们提出的特征注意模块将N个群结构的输出特征融合在一起，这些特征最终传递到重建部分和全局残差学习结构，从而获得无雾输出。此外，每一组结构都将B个基本块结构与局部剩余学习相结合，每一个基本块都结合了跳跃连接和特征注意（FA）模块。FA是由通道注意和像素注意组成的注意机制结构。  </p><blockquote><p>注意力机制:神经网络在进行特征提取的时候，会将所有的输入进行处理，提取得到的特征并没有进行特别的处理。所以注意力机制就是神经网络能够像人一样，并不是“观察”到所有的特征，而是只“注意”到那些真正关心的特征。</p></blockquote><p>本文采用跳跃连接的思想和注意力机制，并设计了一个由多个局部残差学习跳跃连接和特征注意力组成的基本模块。一方面，局部残差学习允许通过多个局部残差学习来绕开薄雾区域的信息和低频信息，使主网络学习更多有用的信息。渠道关注度进一步提高了FFA-Net的功能。</p><h1 id="FFA-Net-结构详解"><a href="#FFA-Net-结构详解" class="headerlink" title="FFA-Net 结构详解"></a>FFA-Net 结构详解</h1><p>FFA-Net主要由三个部分组成:<br>&nbsp;&nbsp;&nbsp;&nbsp;(1) 针对不同通道的特征包含完全不同的加权信息，并且雾度在不同的图像像素上分布不均匀这个问题，提出了一种新的特征注意力（FA）模块，将Channel Attention与Pixel Attention 机制相结合。FA不平等地处理不同的特征和像素，这为处理不同类型的信息提供了额外的灵活性，扩展了CNNs的表示能力。<br>&nbsp;&nbsp;&nbsp;&nbsp;(2) 基本的块结构由local Residual Learning 和特征注意力（FA）模块组成，local Residual Learning 允许通过多个local residual  connections来跳过薄雾区或低频等不太重要的信息，让主网络结构专注于更有效的信息。<br>&nbsp;&nbsp;&nbsp;&nbsp;(3) 基于注意力的不同级别的特征融合（FFA）结构，可以从特征注意力（FA）模块中自适应学习特征权重，从而为重要特征赋予更多权重。这种结构还可以保留浅层信息，并将其传递到深层。  </p><p><a href="https://imgchr.com/i/3eaOWq" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2020/02/20/3eaOWq.png" alt="3eaOWq.png"></a><br>&nbsp;&nbsp;&nbsp;&nbsp;该网络结构主要由三组Group structure，双注意力机制(CA,PA)组成。每个Group structure由N个basic block strcture组成，其中还有跳跃连接。每个basic block strcture内包含特征注意力模块FA（即，Channel Attention (CA) 和Pixel Attention (PA))， 其中还有卷积层，ReLu层，以及跳跃连接。</p><p><a href="https://imgchr.com/i/3ednmD" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2020/02/20/3ednmD.png" alt="3ednmD.png"></a></p><p><a href="https://imgchr.com/i/3edu0e" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2020/02/20/3edu0e.png" alt="3edu0e.png"></a></p><p>以前的基于CNN的图像去雾网络会平等地处理通道方向和像素方向的特征，但是雾度在整个图像上分布不均，非常薄的雾度的权重应与厚的雾度区域像素的权重显着不同。此外，还发现某些像素在至少一个彩色（RGB）通道中具有非常低的强度是很常见的，这进一步说明了不同的通道特征具有完全不同的加权信息。如果我们一视同仁，它将花费大量资源进行次要信息的不必要的计算，网络将无法覆盖所有像素和通道。最后，它将大大限制网络的表示。</p><p>自从注意力机制在神经网络的设计中被广泛使用以来，它在网络性能中发挥了重要作用。受这项工作的启发，本文进一步设计了一个新颖的功能关注（FA）模块。FA模块分别在按通道和按像素的功能中组合了通道注意和像素注意。FA不平等地对待不同的特征和像素，这可以在处理不同类型的信息时提供额外的灵活性。</p><p>ResNet的出现使训练非常深的网络成为可能。本文采用跳过连接的思想和注意力机制，并设计了一个由多个局部残差学习跳过连接和特征注意力组成的基本模块。一方面，局部残差学习允许通过多个局部残差学习来绕开薄雾区域的信息和低频信息，使主网络学习更多有用的信息。渠道关注度进一步提高了FFA-Net的功能。</p><p>随着网络的深入，往往难以保留浅层特征信息。为了识别并融合不同级别的功能，U-Net和其他网络努力整合浅层信息和深层信息。同样，本文提出了一种基于注意力的特征融合结构（FFA），该结构可以保留浅层信息并将其传递到深层。最重要的是，在将所有特征输入特征融合模块之前，FFA-Net对不同级别的特征赋予不同的权重，该权重是通过自适应学习FA模块获得的。它比直接指定重量的要好得多。</p><p>总体而言，本文作者的贡献有以下四个方面：</p><ol><li><p>提出了一种新颖的端到端特征融合关注网络FFA-Net，用于单图像去雾。FFA-Net在很大程度上超越了现有的最新图像去雾方法，在雾度高且纹理细节丰富的区域中，FFA-Net表现尤其出色。</p></li><li><p>提出了一种新颖的功能关注（FA）模块，该模块结合了通道关注和像素关注机制。该模块在处理不同类型的信息时提供了更大的灵活性，将更多的注意力集中在雾度高的像素和更重要的通道信息上。</p></li><li><p>提出了一个由局部残差学习和特征注意（FA）组成的基本块，局部残差学习允许通过多个跳过连接绕过薄雾区域的信息和低频信息，特征注意（FA）进一步提高了FFA-Net的容量。</p></li><li><p>提出了一种基于注意力的特征融合（FFA）结构，该结构可以保留浅层的信息并将其传递到深层。此外，它不仅可以融合所有特征，而且可以自适应地学习不同级别特征信息的不同权重。最后，它比其他特征融合方法具有更好的性能。</p></li></ol><p>尽管FFA-Net结构很简单，但是它比以前的最新方法要好得多，并且具有很大的余量。本文的网络在恢复图像细节和色彩逼真度方面具有强大的优势，有望解决其他低层次的视觉任务，例如排水，超分辨率，去噪。FFA-Net中的FFA和其他有效模块在图像恢复算法中起着重要作用。</p>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>梯度下降</title>
      <link href="/2020/02/06/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
      <url>/2020/02/06/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</url>
      
        <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>通过迭代求出最小值，在神经网络中常用于使损失函数的值最低，从而优化权重参数。根据梯度（导数）的正负，来判断x应该往什么方向取值，函数才能到达最小值。换句话来说，<strong>梯度就是导数，对于多元函数，梯度就是偏导数。</strong> 梯度下降的作用是为了找到函数最小值时所对应的自变量x的值。<strong>注：是为了找自变量x的值。</strong> $$\omega_{n+1}=\omega_n-\sigma\frac{\partial}{\partial\omega}f(\omega_n)$$，其中$\sigma$为学习率。</p><blockquote><p>过程：设损失函数为$J(\omega)=\omega^2$，参数初始值为$\omega=10$，学习率$\sigma=0.2$。首先求得参数$\omega$的梯度为$\nabla=\frac{\partial}{\partial\omega}J(\omega)=2\omega$，之后改变这个参数以使$J(\omega)$的值尽可能小。<br><img src="https://s2.ax1x.com/2020/02/06/16BYtO.jpg" alt="梯度下降算法过程举例"></p></blockquote><h1 id="为什么要使用梯度下降？"><a href="#为什么要使用梯度下降？" class="headerlink" title="为什么要使用梯度下降？"></a>为什么要使用梯度下降？</h1><p>因为对于某些函数（比如多元函数）来说，用导数求最小值计算非常的繁琐，所以才有了梯度下降。</p><h1 id="怎么使用梯度下降？"><a href="#怎么使用梯度下降？" class="headerlink" title="怎么使用梯度下降？"></a>怎么使用梯度下降？</h1><p>如果$\dot{f(x)}&gt;0$,则函数$f(x)$是单调递增的，所以要想得到最小值,自变量$x$必须要变小，也就是往导数增大的反方向移动。同理，如果$\dot{f(x)}&lt;0$,则函数是单调递减的，所以要想得到最小值，自变量$x$必须增大，也就是往导数增大的方向移动。<strong>梯度下降就是通过导数来推算最小值在哪，自变量x在哪。</strong></p><h1 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h1><p>学习率可以直观的理解为每次参数移动的幅度。<strong>设置适当的学习率$\sigma$</strong>对优化过程而言也非常重要。如果学习率过小，虽然能最终到达损失函数的最小值，但这会大大降低模型优化的速度。如果幅度过大，那么可能导致参数的“摆动”特性。</p><h1 id="优化学习率的方法"><a href="#优化学习率的方法" class="headerlink" title="优化学习率的方法"></a>优化学习率的方法</h1><p>Tensorflow的train.py文件内提供了exponential_decay()函数以指数的形式将学习率衰减，还提供了其他一些优化学习率的方法，如<strong>反时限学习率衰减，自然指数学习率衰减，分片常数学习率衰减，多项式学习率衰减。</strong></p>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>损失函数</title>
      <link href="/2020/02/02/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
      <url>/2020/02/02/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h1><blockquote><p>&nbsp;&nbsp;为了训练解决分类问题或回归问题的模型，我们通常会定义一个损失函数来描述对问题的求解精度（用数学的方式刻画预测答案和真实答案之间的距离）。Loss越小，代表模型得到的结果与真实值的偏差越小，也就是说模型越精确。</p></blockquote><h2 id="经典的损失函数"><a href="#经典的损失函数" class="headerlink" title="经典的损失函数"></a>经典的损失函数</h2><blockquote><ol><li>交叉熵损失函数<br>&nbsp;&nbsp;在解决深度学习领域的一些问题时，交叉熵用于刻画两个概率分布向量之间的距离，是<u>分类问题</u>中使用比较广的一种损失函数。<br>$$L=-{\sum_{c=1}^M}y_c\log(p_c)$$<br>其中:<br>&nbsp;&nbsp;- $M$：类别的数量.<br>&nbsp;&nbsp;- ${y_c}$：指示变量（0或1），如果该类别和样本的类别相同就是1，否则是0.<br>&nbsp;&nbsp;- ${p_c}$：对于观测样本属于类别 [公式] 的预测概率.<br>交叉熵计算(by知乎<a href="https://zhuanlan.zhihu.com/p/35709485" target="_blank" rel="noopener">飞鱼说人工智能</a>)<br><img src="https://s2.ax1x.com/2020/02/04/1r90wn.png" alt="交叉熵计算"> </li><li>均方误差损失函数<br>&nbsp;&nbsp;对于回归问题，最常用的损失函数就是均方误差损失函数。<br>$$MSE(y,y^{‘})=\frac{\sum_{i=1}^n(y_i-{y_i^{‘}})^2}{n}$$<br>其中$y_i$为第一个batch中第$i$个数据的答案值，而$y_i^{‘}$也就是神经网络给出的预测值。解决回归问题的网络模型就是以最小化该函数为目标。<br>均方误差计算(by知乎<a href="https://zhuanlan.zhihu.com/p/35709485" target="_blank" rel="noopener">飞鱼说人工智能</a>) <img src="https://s2.ax1x.com/2020/02/04/1rCxu4.png" alt="图一"><br><img src="https://s2.ax1x.com/2020/02/04/1rPSb9.png" alt="图二"></li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>激活函数</title>
      <link href="/2020/02/02/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
      <url>/2020/02/02/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h1><blockquote><p>   如果网络中每个神经元输入为所有输入的加权和，那最后的结果将是整个神经网络成为一个线性模型。将每个神经元（也就是神经网络中的节点）的输出通过一个非线性函数，那么整个<br>神经网络也就不再是线性的了，这个非线性的函数我们通常会称之为“激活函数(Activation Function)”。<br><img src="https://s2.ax1x.com/2020/02/04/1De2FJ.png" alt="神经元"></p></blockquote><h1 id="Why？（为什么要使用激活激活函数函数？）"><a href="#Why？（为什么要使用激活激活函数函数？）" class="headerlink" title="Why？（为什么要使用激活激活函数函数？）"></a>Why？（为什么要使用激活激活函数函数？）</h1><blockquote><p>   如果不用激励函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层节点的输入都是上层输出的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron）了，那么网络的逼近能力就相当有限。正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络表达能力就更加强大（不再是输入的线性组合，而是几乎可以逼近任意函数）。<br>  对与我们来说，通常不可能预先预测出哪种激活函数工作得最好，所以也很难以决定使用哪种激活函数最好。可以通过一些直觉来尝试一些激活函数，如果认为某种隐藏单元可能表现良好，就用它组成神经网络进行训练，最后进行一些评估。</p></blockquote><h2 id="常用的激活函数"><a href="#常用的激活函数" class="headerlink" title="常用的激活函数"></a>常用的激活函数</h2><blockquote><ol><li>ReLu激活函数，这是一个在输入和0之间求最大值的函数。<br>$$ f(z)=max\lbrace0,z\rbrace $$<br>加入ReLU激活函数的神经元被称为整流线性单元。整流线性单元和线性单元非常类似，两者的唯一区别在于整流线性单元在其一半的定义域上输出为0。</li><li>sigmoid激活函数，它能够把输入的连续实值变换为0和1之间的输出，特别的，如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1。<br>$$ f(z)= \frac{1}{1+e^{-{z}}} \quad$$<br>sigmoid函数曾经被使用的很多，不过近年来，用它的人越来越少了。主要是因为它固有的一些 缺点。(1)在深度神经网络中梯度反向传递时导致梯度爆炸和梯度消失，其中梯度爆炸发生的概率非常小，而梯度消失发生的概率比较大。(2)sigmoid 的 output 不是0均值（即zero-centered）。这是不可取的，因为这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。(3)其解析式中含有幂运算，计算机求解时相对来讲比较耗时。对于规模比较大的深度网络，这会较大地增加训练时间。</li><li>tanh激活函数，在引入整流线性单元之前，双曲正切激活函数也经常会被用到，它被定义为：<br>$$ f(z)= \frac{1-e^{-{2z}}}{1+e^{-{2z}}} \quad$$</li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度前馈神经网络</title>
      <link href="/2020/02/01/%E6%B7%B1%E5%BA%A6%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2020/02/01/%E6%B7%B1%E5%BA%A6%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><blockquote><p>定义：深度前馈神经网络，可简称为前馈神经网络，指的是具有前馈特征的一类神经网络模型。实际上深度神经网络应该泛指更多的使用了深度学习技术的深度神经网络模型。</p></blockquote><h2 id="网络的前馈方式"><a href="#网络的前馈方式" class="headerlink" title="网络的前馈方式"></a>网络的前馈方式</h2><blockquote><p>前馈神经网络模型是向前的，在模型的输出和模型本身之间并不存在连接，也就不构成反馈。例如，对于一个分类器功能的MLP，假设其输入，输出满足一个函y=f(x)(其中x是原始数据的载体，也就是网络的输入)，信息从输入的x经过定义的功能函数f,最终到达输出y。在这个过程中，f以及x并没有因为y的取值而受到任何影响。</p></blockquote><h2 id="全连接"><a href="#全连接" class="headerlink" title="全连接"></a>全连接</h2><blockquote><p>全连接与稀疏连接都指的是神经网络模型中相邻两层单元间的连接方式。<br>全部使用全连接方式的网络，通常成为全连神经网络；仅某一层由全连方式得到。则称之为全连层。MLP就是一个全连神经网络，卷积神经网络则是一个使用了稀疏连接的典型网络。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
          <category> 深度学习 </category>
          
          <category> 神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2020/02/01/hello-world/"/>
      <url>/2020/02/01/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
